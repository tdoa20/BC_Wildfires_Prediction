# -*- coding: utf-8 -*-
"""subset regression_Tolu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aMwt6N1-c6FGO0LY8NMllC0koUdYO-s5
"""

# Importing pandas for working with tabular data (like spreadsheets or CSV files)
import pandas as pd

# Importing NumPy for numerical operations and handling arrays
import numpy as np

# Importing Matplotlib for creating basic plots and graphs
import matplotlib.pyplot as plt

# Importing Seaborn for creating more attractive and informative statistical visualizations
import seaborn as sns

# Importing statsmodels for advanced statistical modeling (e.g., regression analysis)
import statsmodels.api as sm

# Importing math library for basic mathematical functions (e.g., square root, log)
import math

# Importing OS library to interact with the operating system (e.g., file paths, directories)
import os

# Importing tools for splitting data and validating models
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold

# Importing StandardScaler to normalize (standardize) numeric features
from sklearn.preprocessing import StandardScaler

# Importing logistic regression model for binary classification
from sklearn.linear_model import LogisticRegression

# Importing Random Forest algorithm for classification tasks
from sklearn.ensemble import RandomForestClassifier

# Importing various metrics to evaluate model performance
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score

# Importing XGBoost classifier, a powerful tree-based model used in many competitions and real-world applications
from xgboost import XGBClassifier

# More metrics to evaluate predictions (e.g., confusion matrix, accuracy, precision, recall)
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score


"""### load data"""

base_dir = "C:/Users/tdoa2/Downloads/Spatial data analysis"

# Load data
df_list = []
for year in range(2000, 2025):
  if year % 2 == 0 and year != 2024:
    df_temp = pd.read_csv(os.path.join(base_dir, f"Spatial data cleaning/Point_data/Sampled/Combined_Sampled_Points_{year}-{year+1}.csv"))
    df_list.append(df_temp)
  elif year == 2024:
    df_temp = pd.read_csv(os.path.join(base_dir, f"Spatial data cleaning/Point_data/Sampled/Combined_Sampled_Points_{year}.csv"))
    df_list.append(df_temp)
  else:
      continue

df = pd.concat(df_list, ignore_index=True)

# Drop missing values
initial_rows = len(df)
df = df.dropna()
final_rows = len(df)

print(f"Number of rows before dropping missing values: {initial_rows}")
print(f"Number of rows after dropping missing values: {final_rows}")


# Create dummy variables for 'Fuel_Type'
fuel_dummies = pd.get_dummies(df['Fuel_Type'], prefix='Fuel', drop_first=True)
df = pd.concat([df, fuel_dummies], axis=1)
df.drop('Fuel_Type', axis=1, inplace=True)

# Convert boolean columns to integers (assuming boolean columns exist)
for col in df.select_dtypes(include=['bool']):
    df[col] = df[col].astype(int)

print(df.tail())

df.info() # Verify the existence of the dummies and the data types

# Wind Speed Calculation
df['Wind_Speed'] = np.sqrt(np.abs(df['u10_wind'] + df['v10_wind']))


df.columns   #for copy and paste

# Create columns with the columns of df. Let X be all the columns except 'Fire' in df and y be the 'Fire' column

X = df.drop(['Fire', 'X', 'Y', 'Month', 'Year', 'Latitude', 'Longitude', 'u10_wind', 'v10_wind'], axis=1)
y = df['Fire']

print(X.columns)

print('number of fire: ', len(y[y==1]), '\nnumber of nonfire: ', len(y[y==0]))

"""### split"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

print('number of train and test: ', len(y_train), ', ', len(y_test), '\nnumber of fire in train: ', len(y_train[y_train==1]),
      '\nnumber of fire in test: ', len(y_test[y_test==1]))

"""### Logistic Regression"""

logreg = LogisticRegression()
logreg.fit(X_train, y_train)

# Get predicted probabilities
logreg_probs = logreg.predict_proba(X_train)[:, 1]

# Create a statsmodels Logit object for summary
X_train_const = sm.add_constant(X_train)  # Add constant for intercept
logit_model = sm.Logit(y_train, X_train_const)
result = logit_model.fit()

# Print the summary
print(result.summary())

logreg_probs = logreg.predict_proba(X_test)[:, 1]
print(logreg_probs)

print("Logistic Regression AUC:", roc_auc_score(y_test, logreg_probs))


"""#### cross validation"""

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Cross-validated AUC Scores
cv_auc_scores = cross_val_score(logreg, X, y, cv=skf, scoring='roc_auc')

print(f"Logistic Regression Stratified CV AUC Scores: {cv_auc_scores}")
print(f"Logistic Regression Average CV AUC: {np.mean(cv_auc_scores)}")

plt.figure(figsize=(6, 5))
plt.boxplot(cv_auc_scores, vert=False)
plt.title('Logistic Regression Stratified 5-Fold CV AUC')
plt.xlabel('AUC Score')
plt.grid(True)
plt.show()

"""### Random Forest"""

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Get predicted probabilities for testing data
rf_probs = rf.predict_proba(X_test)[:, 1]

print("Random Forest AUC:", roc_auc_score(y_test, rf_probs))

# Feature importance for Random Forest Model
feature_importance = rf.feature_importances_
features = pd.DataFrame(X).columns

# DataFrame for feature importances
importance_df_rf = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importance
}).sort_values(by='Importance', ascending=False)

print(importance_df_rf)

# Save dataframe as a CSV file
importance_df_rf.to_csv(os.path.join(base_dir, "Model Analysis/Graphs/Random_Forest_Feature_Importance.csv"), index=False)

# Python feature importance plot
plt.figure(figsize=(8, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df_rf)
plt.title('Feature Importance (RF)')
plt.tight_layout()
plt.show()

"""#### cross validation"""

rf_model = RandomForestClassifier(random_state=42)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Cross-validated AUC Scores
cv_auc_scores = cross_val_score(rf_model, X, y, cv=skf, scoring='roc_auc')

print(f"Random Forest Stratified CV AUC Scores: {cv_auc_scores}")
print(f"Random Forest Average CV AUC: {np.mean(cv_auc_scores)}")

plt.figure(figsize=(6, 5))
plt.boxplot(cv_auc_scores, vert=False)
plt.title('Random Forest Stratified 5-Fold CV AUC')
plt.xlabel('AUC Score')
plt.grid(True)
plt.show()

"""### XGBOOST"""

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb.fit(X_train, y_train)

# Get predicted probabilities for testing data
xgb_probs = xgb.predict_proba(X_test)[:, 1]

print("XGBoost AUC:", roc_auc_score(y_test, xgb_probs))

# Feature importance for XGBoost Model
feature_importance = xgb.feature_importances_
features = pd.DataFrame(X).columns

# DataFrame for feature importances
importance_df_xgb = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importance
}).sort_values(by='Importance', ascending=False)

print(importance_df_xgb)

# Save dataframe as CSV
importance_df_xgb.to_csv(os.path.join(base_dir, "Model Analysis/Graphs/XGBoost_Feature_Importance.csv"), index=False)

# Python feature importance plot
plt.figure(figsize=(8, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df_xgb)
plt.title('Feature Importance (XGBoost)')
plt.tight_layout()
plt.show()

"""#### cross validation"""

xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Cross-validated AUC Scores
cv_auc_scores = cross_val_score(xgb_model, X, y, cv=skf, scoring='roc_auc')

print(f"XGBoost Stratified CV AUC Scores: {cv_auc_scores}")
print(f"XGBoost Average CV AUC: {np.mean(cv_auc_scores)}")

plt.figure(figsize=(6, 5))
plt.boxplot(cv_auc_scores, vert=False)
plt.title('XGBoost Stratified 5-Fold CV AUC')
plt.xlabel('AUC Score')
plt.grid(True)
plt.show()


# Accuracy, Sensitivity, Recall, RMSE for all models
rf_predictions = rf.predict(X_test)
xgboost_predictions = xgb.predict(X_test)
log_reg_predictions = logreg.predict(X_test)


# Random Forest
rf_accuracy = accuracy_score(y_test, rf_predictions)
rf_precision = precision_score(y_test, rf_predictions, zero_division=0)
rf_recall = recall_score(y_test, rf_predictions, zero_division=0)  # Same as sensitivity
rf_rmse = math.sqrt(np.mean((y_test - rf_predictions)**2))
print(f"Random Forest Accuracy: {rf_accuracy}")
print(f"Random Forest Precision: {rf_precision}")
print(f"Random Forest Recall (Sensitivity): {rf_recall}")
print(f"Root Mean Squared Error (RMSE) for Random Forest predictions: {rf_rmse}")

# XGBoost
xgb_accuracy = accuracy_score(y_test, xgboost_predictions)
xgb_precision = precision_score(y_test, xgboost_predictions, zero_division=0)
xgb_recall = recall_score(y_test, xgboost_predictions, zero_division=0)  # Same as sensitivity
xgb_rmse = math.sqrt(np.mean((y_test - xgboost_predictions)**2))
print(f"XGBoost Accuracy: {xgb_accuracy}")
print(f"XGBoost Precision: {xgb_precision}")
print(f"XGBoost Recall (Sensitivity): {xgb_recall}")
print(f"Root Mean Squared Error (RMSE) for XGBoost predictions: {xgb_rmse}")


# Logistic Regression
logreg_accuracy = accuracy_score(y_test, log_reg_predictions)
logreg_precision = precision_score(y_test, log_reg_predictions, zero_division=0)
logreg_recall = recall_score(y_test, log_reg_predictions, zero_division=0)  # Same as sensitivity
logreg_rmse = math.sqrt(np.mean((y_test - log_reg_predictions)**2))
print(f"Logistic Regression Accuracy: {logreg_accuracy}")
print(f"Logistic Regression Precision: {logreg_precision}")
print(f"Logistic Regression Recall (Sensitivity): {logreg_recall}")
print(f"Root Mean Squared Error (RMSE) for Logistic Regression predictions: {logreg_rmse}")



# Confusion Matrix for Random Forest test points
cm = confusion_matrix(y_test, rf_predictions)

cm_df = pd.DataFrame(cm,
                     index = ['Actual Non-Fire Points', 'Actual Fire Points'],
                     columns = ['Predicted Non-Fire Points', 'Predicted Fire Points'])

cm_df.to_csv(os.path.join(base_dir, "Model Analysis/Tables/Random_Forest_Confusion_Matrix.csv"), index=False)



# Filtered August 2024 table for confusion matrix

filtered_df = df[(df['Month'] == 'August') & (df['Year'] == 2024)]
filtered_X = filtered_df.drop(['Fire', 'X', 'Y', 'Month', 'Year', 'Latitude', 'Longitude', 'u10_wind', 'v10_wind'], axis=1)
filtered_y = filtered_df['Fire']
print(filtered_df)

rf.fit(filtered_X, filtered_y)
filtered_y_pred = rf.predict(filtered_X)
filtered_predict_prob =  rf.predict_proba(filtered_X)[:, 1]

filtered_table = pd.DataFrame({
    'Longitude': filtered_df['Longitude'],
    'Latitude': filtered_df['Latitude'],
    'Predicted Probability': filtered_predict_prob,
    'Predicted Fire': filtered_y_pred,  # Assuming y_pred is already calculated
    'Actual Fire': filtered_df['Fire'] # Aligns with X_train index
})


# Save as CSV
filtered_table.to_csv(os.path.join(base_dir, "Spatial data cleaning/Point_data/August_2024_Fires.csv"), index=False)


# Calculate the confusion matrix based on 'Actual Fire' and 'Predicted Fire' columns in filtered_table
conf_matrix = confusion_matrix(filtered_table['Actual Fire'], filtered_table['Predicted Fire'])

# Convert the confusion matrix to a pandas DataFrame for better readability
conf_matrix_df = pd.DataFrame(conf_matrix,
                              index=['Actual 0', 'Actual 1'],
                              columns=['Predicted 0', 'Predicted 1'])


# Print the confusion matrix DataFrame
print("Confusion Matrix (DataFrame):")
print(conf_matrix_df)


# PARTIAL INDEPENDENCE PLOTS
# Random forest
def plot_variable_effect(rf_model, X, feature_index, feature_name=None, num_points=100, save_csv_path=None):
    """
    Plots and returns the effect of a feature on average predicted fire probability.
    Optionally saves to CSV for use in Power BI.
    """
    X_temp = X.copy()

    # Generate x-axis values
    if isinstance(X, pd.DataFrame):
        x_min, x_max = X.iloc[:, feature_index].min(), X.iloc[:, feature_index].max()
    else:
        x_min, x_max = X[:, feature_index].min(), X[:, feature_index].max()
    x_values = np.linspace(x_min, x_max, num_points)

    y_probs = []
    for val in x_values:
        if isinstance(X, pd.DataFrame):
            X_temp.iloc[:, feature_index] = val
        else:
            X_temp[:, feature_index] = val

        probs = rf_model.predict_proba(X_temp)[:, 1]
        y_probs.append(np.mean(probs))

    # Combine into DataFrame
    df = pd.DataFrame({
        feature_name or f"Feature_{feature_index}": x_values,
        "Avg_Predicted_Prob": y_probs
    })

    # Save to CSV if path is given
    if save_csv_path:
        df.to_csv(save_csv_path, index=False)
        print(f"✅ CSV saved to: {save_csv_path}")

    # Plot
    plt.figure(figsize=(8, 5))
    plt.plot(x_values, y_probs, color='navy')
    plt.xlabel(feature_name or f'Feature {feature_index}')
    plt.ylabel('Average Predicted Fire Probability')
    plt.title(f'Effect of {feature_name or f"Feature {feature_index}"} on Fire Probability')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    return df

# 2m Temperature effect Random Forest
feature_index = list(X_train.columns).index('temp_2m')
print(feature_index)
csv_path = os.path.join(base_dir, "Model Analysis/Graphs/RF_temp_2m_avg_probabilities.csv")

df_effect = plot_variable_effect(
    rf_model=rf,
    X=X_train,
    feature_index=feature_index,
    feature_name="2-metre temperature (K)",
    save_csv_path=csv_path
)

# Dewpoint temperature
feature_index = list(X_train.columns).index('dew_temp_2')
print(feature_index)
csv_path = os.path.join(base_dir, "Model Analysis/Graphs/RF_dewpoint_temp_avg_probabilities.csv")

df_effect = plot_variable_effect(
    rf_model=rf,
    X=X_train,
    feature_index=feature_index,
    feature_name="2-metre dewpoint temperature (K)",
    save_csv_path=csv_path
)


# Total precipitation
feature_index = list(X_train.columns).index('tot_precip')
print(feature_index)
csv_path = os.path.join(base_dir, "Model Analysis/Graphs/RF_total_precipitation_avg_probabilities.csv")

df_effect = plot_variable_effect(
    rf_model=rf,
    X=X_train,
    feature_index=feature_index,
    feature_name="Total Precipitation (m)",
    save_csv_path=csv_path
)


# LAI High
feature_index = list(X_train.columns).index('lai_high')
print(feature_index)
csv_path = os.path.join(base_dir, "Model Analysis/Graphs/RF_lai_high_avg_probabilities.csv")

df_effect = plot_variable_effect(
    rf_model=rf,
    X=X_train,
    feature_index=feature_index,
    feature_name="High leaf Agriculture Index",
    save_csv_path=csv_path
)


# Wind Speed
feature_index = list(X_train.columns).index('Fuel_119')
print(feature_index)
csv_path = os.path.join(base_dir, "Model Analysis/Graphs/RF_Wind_Speed_avg_probabilities.csv")

df_effect = plot_variable_effect(
    rf_model=rf,
    X=X_train,
    feature_index=feature_index,
    feature_name="Wind Speed",
    save_csv_path=csv_path
)

# Fuel Type 119
feature_index = list(X_train.columns).index('Wind_Speed')
print(feature_index)
csv_path = os.path.join(base_dir, "Model Analysis/Graphs/RF_Wind_Speed_avg_probabilities.csv")

df_effect = plot_variable_effect(
    rf_model=rf,
    X=X_train,
    feature_index=feature_index,
    feature_name="Fuel Type 119",
    save_csv_path=csv_path
)

